{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e18deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bertopic llama-cpp-python sentence-transformers umap-learn hdbscan datamapplot\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "from llama_cpp import Llama\n",
    "from bertopic.representation import KeyBERTInspired, LlamaCPP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "import datamapplot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for japanese fonts in  plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Noto Sans CJK JP\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# Detect device: Apple MPS if available, else CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# One‐time NLTK download\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "\n",
    "#Upload japanese processor and topic modeler (defined manuallt in multiling_helpers.py)\n",
    "import sys\n",
    "script_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "print(\"Script directory:\", script_dir)\n",
    "multilingual_dir = os.path.join(script_dir, 'MULTILINGUAL')\n",
    "sys.path.append(multilingual_dir)\n",
    "print(f\"Added {multilingual_dir} to Python path\")\n",
    "from multiling_helpers import JapaneseProcessor, TopicModeler\n",
    "\n",
    "#initialise japanese processor\n",
    "japanese_processor = JapaneseProcessor()\n",
    "japanese_processor.sentence_transformer_model = \"sonoisa/sentence-bert-base-ja-mean-tokens\"#\"paraphrase-multilingual-mpnet-base-v2\"#\"sonoisa/sentence-bert-base-ja-mean-tokens\" #\"oshizo/sbert-jsnli-luke-japanese-base-lite\" #\"sonoisa/sentence-bert-base-ja-mean-tokens\" #cl-tohoku/bert-base-japanese-v3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216806f3",
   "metadata": {},
   "source": [
    "### download and configure Japanese Llama 3 (Eliza)\n",
    "\n",
    "https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── CONFIG ──\n",
    "repo_id   = \"elyza/Llama-3-ELYZA-JP-8B-GGUF\"\n",
    "filename  = \"Llama-3-ELYZA-JP-8B-q4_k_m.gguf\"\n",
    "cache_dir = \"/Users/rbeaute/Projects/MOSAIC/MULTILINGUAL/models/elyza_Llama-3-ELYZA-JP-8B\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ── DOWNLOAD ──\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=filename,\n",
    "    cache_dir=cache_dir,\n",
    "    repo_type=\"model\",\n",
    "    force_filename=filename\n",
    ")\n",
    "print(\"Quantized GGUF model saved to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add273ae",
   "metadata": {},
   "source": [
    "### Load innerspeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_pkl = \"/Users/rbeaute/Projects/MOSAIC/DATA/multilingual/japanese/innerspeech/innerspeech_reports.pkl\"\n",
    "\n",
    "with open(local_pkl, \"rb\") as f:\n",
    "    raw_reports = pickle.load(f)\n",
    "\n",
    "print(\"Raw reports type:\", type(raw_reports))\n",
    "if hasattr(raw_reports, \"head\"):\n",
    "    display(raw_reports.head(3))\n",
    "    docs = raw_reports[\"text\"].astype(str).tolist()  \n",
    "else:\n",
    "    docs = [str(x) for x in raw_reports]\n",
    "    print(\"Sample docs:\", docs[:3])\n",
    "\n",
    "print(f\"Total documents: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d4e5",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = [japanese_processor.preprocess_text(d) for d in docs] #super basic preprocessing (remove extra spaces, etc)\n",
    "sentences, doc_map = japanese_processor.split_sentences(cleaned_docs)\n",
    "print(f\"Split into {len(sentences)} sentences from {len(cleaned_docs)} docs\")\n",
    "\n",
    "print(\"First 3 raw doc:\", docs[:3])\n",
    "# print(\"First 5 cleaned docs:\", cleaned_docs[:10])\n",
    "print(\"First 5 sentences:\", sentences[:5])\n",
    "print(\"doc_map for those 5 sentences:\", doc_map[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    \"raw\": docs[:5],\n",
    "    \"cleaned\": cleaned_docs[:5]\n",
    "})\n",
    "print(sample_df.to_markdown())\n",
    "\n",
    "for i, sent in enumerate(sentences[:5]):\n",
    "    print(f\"{i:02d}:\", sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAW docs[0]:\", repr(docs[0]))\n",
    "print(\"CLEANED docs[0]:\", repr(cleaned_docs[0]))\n",
    "for i, sent in enumerate(sentences[:5]):\n",
    "    print(f\"  sentence[{i}]:\", repr(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23978f84",
   "metadata": {},
   "source": [
    "###  Instantiate LLM & embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab69fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized Llama-3 (runs on CPU; no CUDA)\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Sentence-Transformer on MPS/CPU\n",
    "embed_model = SentenceTransformer(\n",
    "    japanese_processor.sentence_transformer_model,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fee11",
   "metadata": {},
   "source": [
    "### Compute Embeddings & 2D Viz Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_model.encode(sentences, show_progress_bar=True)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "viz_embeddings = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=2,\n",
    "    min_dist=0.5,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ").fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703a86c",
   "metadata": {},
   "source": [
    "### Define UMAP/HDBSCAN & Representation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=10,\n",
    "    min_dist=0.01, # > 0 to avoid overfitting and avoid collapsing points too tightly\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10, #allow to form topics out of 10 sentences\n",
    "    min_samples=5, \n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_epsilon=0.1, #help merge closer clusters\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Q:  \n",
    "# トピックのキーワード: [KEYWORDS]  \n",
    "# トピックのドキュメント: [DOCUMENTS]  \n",
    "\n",
    "# 以下の条件を満たす科学的なラベルを生成してください:  \n",
    "# 1. **タイトルケース**（例: 「Adhd 診断 併存」）  \n",
    "# 2. **2～4語以内**（名詞のみ、動詞・形容詞は不可）  \n",
    "# 3. キーワード/ドキュメントの**明示的な用語のみ反映**（解釈や抽象表現は禁止）  \n",
    "# 4. 句読点、例文、説明文は一切含まない  \n",
    "\n",
    "# A:  \n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Q:\n",
    "以下の情報をもとに、科学的な「トピックラベル」を生成してください。\n",
    "\n",
    "――――――\n",
    "\n",
    "サンプル:\n",
    "トピックのキーワード: [「内言」「映像」「味」]\n",
    "トピックのドキュメント: \n",
    "「頭の中では日本語で考えているとき、文字か音声かではなく映像や味も感じることがある。」\n",
    "\n",
    "生成ラベル: 「内言 感覚結合」\n",
    "\n",
    "――――――\n",
    "\n",
    "本番:\n",
    "トピックのキーワード: [KEYWORDS]\n",
    "トピックのドキュメント: [DOCUMENTS]\n",
    "\n",
    "以下の条件を満たす科学的なラベルを生成してください:\n",
    "1. タイトルケース（例: 「Adhd 診断 併存」）\n",
    "2. 2～4語以内（名詞のみ、動詞・形容詞は不可）\n",
    "3. キーワード/ドキュメントの明示的な用語のみ反映（解釈や抽象表現は禁止）\n",
    "4. 句読点、例文、説明文は一切含まない\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    \"max_tokens\": 6,               # ラベル長を制限（4語 + 余白）\n",
    "    \"temperature\": 0.1,            # 低ランダム性（キーワード厳守）\n",
    "    \"top_p\": 0.6,                  # 高確率トークンのみサンプリング\n",
    "    \"repeat_penalty\": 1.2,         # 単語繰り返しを軽減\n",
    "    \"stop\": [\"\\n\", \"。\", \"、\"]     # 1行のみ出力\n",
    "}\n",
    "\n",
    "# representation_model = {\n",
    "#     \"KeyBERT\": KeyBERTInspired(),\n",
    "#     \"LLM-JP\": LlamaCPP(llm, prompt=prompt)\n",
    "# }\n",
    "\n",
    "\n",
    "representation_model = {\n",
    "   \"KeyBERT\": KeyBERTInspired(),\n",
    "   \"LLM\": LlamaCPP(llm, prompt=prompt,nr_docs=10, #show 10 sentences per topic\n",
    "                   pipeline_kwargs=pipeline_kwargs,diversity=0.2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(japanese_processor.stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d2ecf",
   "metadata": {},
   "source": [
    "### Fit Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what a vectoriser would do \n",
    "temp_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=list(japanese_processor.stopwords),    # no stopwords\n",
    "    max_df=0.85\n",
    ")\n",
    "temp_vectorizer.fit(sentences)\n",
    "print(\"Temp vocabulary size :\", len(temp_vectorizer.vocabulary_))\n",
    "print(\"Sample words :\", list(temp_vectorizer.vocabulary_.keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embed_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    # vectorizer_model=CountVectorizer(\n",
    "    #     ngram_range=(1,2),  # bigrams \n",
    "    #     stop_words=list(japanese_processor.stopwords),\n",
    "    #     max_df=0.85, #lower to drop excessively common words\n",
    "    #     min_df=2), #avoid gettinh topics on very rare words\n",
    "    language=\"japanese\",\n",
    "    top_n_words=10,\n",
    "    nr_topics=\"auto\", #automatically reduce number of topics by merging similar ones\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "topics, probs = topic_model.fit_transform(sentences, embeddings)\n",
    "print(\"Number of topics:\", len(set(topics)) - (1 if -1 in topics else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad002db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print raw labels exytavyed by LLama\n",
    "\n",
    "raw_llm_labels = [label[0][0] for label in topic_model.get_topics(full=True)[\"LLM\"].values()]\n",
    "print(raw_llm_labels)\n",
    "\n",
    "llm_labels = raw_llm_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need more cleaning (as we did for english), can use:\n",
    "\n",
    "# import re \n",
    "\n",
    "# llm_labels = [label[0][0].replace('\\nThe topic is labeled as:','').replace('\\n', '').replace('Label:', '').replace('\"', '') for label in topic_model.get_topics(full=True)[\"LLM\"].values()]\n",
    "# llm_labels\n",
    "# llm_labels = [re.sub(r'\\W+', ' ', label[0][0].split(\"\\n\")[0].replace('\"', '')) for label in topic_model.get_topics(full=True)[\"LLM\"].values()]\n",
    "# llm_labels = [label if label else \"Unlabelled\" for label in llm_labels]\n",
    "# all_labels = [llm_labels[topic+topic_model._outliers] if topic != -1 else \"Unlabelled\" for topic in topics] \n",
    "\n",
    "# filtered_labels = [label for label in all_labels if label != \"Unlabelled\"] #remove -1 topics (outliers)\n",
    "\n",
    "# llm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d3fdd",
   "metadata": {},
   "source": [
    "### Visualise topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f14ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the summary table of all topics (including the ‘–1’ outliers)\n",
    "info = topic_model.get_topic_info()\n",
    "display(info)\n",
    "\n",
    "# build a map from topic ID to label \n",
    "name_map = dict(zip(info.Topic, info.Name))\n",
    "\n",
    "# for each document look its label\n",
    "bertopic_labels = [ name_map[t] for t in topics ]\n",
    "np.unique(bertopic_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1880491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import datamapplot\n",
    "\n",
    "# force reload so create_plot is back to its original\n",
    "datamapplot = importlib.reload(datamapplot)\n",
    "\n",
    "_orig_create = datamapplot.create_plot\n",
    "\n",
    "\n",
    "from matplotlib.text import Text, Annotation\n",
    "\n",
    "def create_jp_plot(*args, font=\"Hiragino Sans\", **kwargs):\n",
    "    # draw with the genuine original\n",
    "    fig, ax = _orig_create(*args, **kwargs)\n",
    "    # patch every Text and Annotation\n",
    "    for art in fig.findobj(match=lambda o: isinstance(o, (Text, Annotation))):\n",
    "        try:\n",
    "            art.set_fontname(font)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return fig, ax\n",
    "\n",
    "datamapplot.create_plot = create_jp_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = datamapplot.create_plot(\n",
    "    viz_embeddings,\n",
    "    bertopic_labels,\n",
    "    label_font_size=14,\n",
    "    # title=\"Japanese BERTopic (Llama-3-ELYZA-JP-8B)\",\n",
    "    # sub_title=\"Labels by quantized Llama-3-ELYZA\",\n",
    "    label_wrap_width=15,\n",
    "    use_medoids=False,\n",
    "    figsize=(30, 25)\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbda2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
