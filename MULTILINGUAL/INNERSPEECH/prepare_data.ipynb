{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd663e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOX_ROOT  : /Users/rb666/Library/CloudStorage/Box-Box/TMDATA\n",
      "RAW_DIR   : /Users/rb666/Library/CloudStorage/Box-Box/TMDATA/INNERSPEECH\n",
      "LOCAL_DATA: /Users/rb666/Projects/MOSAIC/DATA/innerspeech\n",
      "PREPROC   : /Users/rb666/Projects/MOSAIC/DATA/innerspeech/preprocessed\n",
      "CACHE_DIR : /Users/rb666/Projects/MOSAIC/DATA/innerspeech/preprocessed/cache\n"
     ]
    }
   ],
   "source": [
    "# --- paths & setup (portable) ---\n",
    "from pathlib import Path\n",
    "import os, json, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from mosaic.path_utils import CFG, raw_path, proc_path, project_root\n",
    "\n",
    "# RAW Box folder name for this dataset\n",
    "DATASET_RAW = \"INNERSPEECH\"   \n",
    "# Processed target: ~/.../DATA/innerspeech/preprocessed\n",
    "LOCAL_DATA_DIR = proc_path(str(DATASET_RAW).lower())\n",
    "PREPROC_DIR = proc_path(str(DATASET_RAW).lower(), \"preprocessed\")\n",
    "CACHE_DIR   = PREPROC_DIR / \"cache\"\n",
    "\n",
    "# (optional) repo-root helpers if want to import local modules\n",
    "ROOT = project_root()\n",
    "\n",
    "# make sure dirs exist\n",
    "PREPROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_DIR = raw_path(DATASET_RAW)\n",
    "\n",
    "print(\"BOX_ROOT  :\", CFG[\"box_root\"])\n",
    "print(\"RAW_DIR   :\", RAW_DIR)\n",
    "print(\"LOCAL_DATA:\", LOCAL_DATA_DIR)\n",
    "print(\"PREPROC   :\", PREPROC_DIR)\n",
    "print(\"CACHE_DIR :\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17db39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: /Users/rb666/Projects/MOSAIC/DATA/innerspeech/innerspeech_reflection_reports.csv\n",
      "Loaded 731 reports from /Users/rb666/Projects/MOSAIC/DATA/innerspeech/innerspeech_reflection_reports.csv\n",
      "(731, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reflection_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>頭の中の独り言をこのような調査で改めて自覚することができ、また色々なパターンがあることを知り...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>他人の声が脳内でしている人がいるという話にすごく興味があるのですが、心理物理実験で音声のパラ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>頭の中では日本語で考えているという自覚はある（英語は勉強以外にほぼ使ったことはない）が、文字...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>自動思考というものなのか、直近で起きた失敗などを批判する考えが勝手に浮かんできたりすることが...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>食べたいもの、欲しいものなどは、自分がそれを食べている、或いは使っているところを想像して決め...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   reflection_answer\n",
       "0  頭の中の独り言をこのような調査で改めて自覚することができ、また色々なパターンがあることを知り...\n",
       "1  他人の声が脳内でしている人がいるという話にすごく興味があるのですが、心理物理実験で音声のパラ...\n",
       "2  頭の中では日本語で考えているという自覚はある（英語は勉強以外にほぼ使ったことはない）が、文字...\n",
       "3  自動思考というものなのか、直近で起きた失敗などを批判する考えが勝手に浮かんできたりすることが...\n",
       "4  食べたいもの、欲しいものなどは、自分がそれを食べている、或いは使っているところを想像して決め..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load reflection reports CSV ---\n",
    "csv_path = os.path.join(LOCAL_DATA_DIR, f\"{str(DATASET_RAW).lower()}_reflection_reports.csv\")\n",
    "print(\"CSV:\", csv_path)\n",
    "\n",
    "if not Path(csv_path).exists():\n",
    "    raise FileNotFoundError(f\"Missing file: {csv_path}\")\n",
    "\n",
    "#load only the reflection_answers column\n",
    "df = pd.read_csv(csv_path, usecols=[\"reflection_answer\"])\n",
    "n_reports = df.shape[0]\n",
    "print(f\"Loaded {n_reports} reports from {csv_path}\")\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195b41a",
   "metadata": {},
   "source": [
    "### Sample translate .csv file multilangual into English (using Gemini API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366dc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- SETUP AND CONFIGURATION ---\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the GOOGLE_API_KEY in your .env file.\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "try:\n",
    "    csv_path = '/Users/rbeaute/Projects/MOSAIC/DATA/multilingual/japanese/innerspeech/innerspeech_reflection_reports.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Successfully loaded {len(df)} rows to be translated from {csv_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file could not be found at {csv_path}\")\n",
    "    df = pd.DataFrame({'reflection_answer': []}) # Create empty df if file not found\n",
    "\n",
    "# --- BATCH TRANSLATION FUNCTION WITH EXPONENTIAL BACKOFF ---\n",
    "\n",
    "def translate_batch_with_retry(texts: list[str], max_retries: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Translates a BATCH of texts, with automatic retries for rate limit errors.\n",
    "    \"\"\"\n",
    "    numbered_texts = \"\\\\n\".join([f'\"{i+1}\": \"{text}\"' for i, text in enumerate(texts)])\n",
    "    prompt = f\"\"\"Translate each of the following numbered Japanese texts to English.\n",
    "Please return the result as a single, valid JSON object where keys are the numbers and values are the English translations.\n",
    "The JSON object should have exactly {len(texts)} elements. Do not include any other explanatory text in your response.\n",
    "\n",
    "TEXTS TO TRANSLATE:\n",
    "{{\n",
    "{numbered_texts}\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            cleaned_response_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            translated_dict = json.loads(cleaned_response_text)\n",
    "            translated_texts = [translated_dict.get(str(i+1), \"Error: Missing translation\") for i in range(len(texts))]\n",
    "\n",
    "            if len(translated_texts) == len(texts):\n",
    "                return translated_texts\n",
    "            else:\n",
    "                return [\"Error: Mismatch in batch response\"] * len(texts)\n",
    "\n",
    "        except exceptions.ResourceExhausted as e:\n",
    "            print(f\"Rate limit exceeded. Waiting to retry... (Attempt {attempt + 1}/{max_retries})\")\n",
    "            retry_after = 15 * (2 ** attempt) + random.uniform(0, 1) # Exponential backoff with jitter\n",
    "            print(f\"Waiting for {retry_after:.2f} seconds.\")\n",
    "            time.sleep(retry_after)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during a batch translation: {e}\")\n",
    "            return [f\"Error: {e}\"] * len(texts)\n",
    "    \n",
    "    print(\"All retries failed for this batch.\")\n",
    "    return [\"Error: Max retries exceeded\"] * len(texts)\n",
    "\n",
    "# --- EXECUTE THE BATCHED TRANSLATION ---\n",
    "if not df.empty:\n",
    "    BATCH_SIZE = 20\n",
    "    all_translations = []\n",
    "\n",
    "    text_batches = np.array_split(df['reflection_answer'].dropna(), len(df) // BATCH_SIZE + 1)\n",
    "    print(f\"Split {len(df)} entries into {len(text_batches)} batches of up to {BATCH_SIZE} each.\")\n",
    "\n",
    "    for batch in tqdm(text_batches, desc=\"Translating Batches\"):\n",
    "        if batch.empty:\n",
    "            continue\n",
    "        translations = translate_batch_with_retry(batch.tolist())\n",
    "        all_translations.extend(translations)\n",
    "        time.sleep(1)\n",
    "\n",
    "    df['reflection_answer_english'] = pd.Series(all_translations)\n",
    "    \n",
    "    # --- REVIEW AND SAVE RESULTS ---\n",
    "    print(\"\\\\n--- Translation Results (First 5 Rows) ---\")\n",
    "    print(df[['reflection_answer', 'reflection_answer_english']].head())\n",
    "\n",
    "    output_path = '/Users/rbeaute/Projects/MOSAIC/DATA/multilingual/innerspeech_translated_batched.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\\\nTranslated data saved to {output_path}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, skipping translation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(\"Original:\", df.loc[i, 'reflection_answer'])\n",
    "    print(\"Translated:\", df.loc[i, 'reflection_answer_english'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1bf47",
   "metadata": {},
   "source": [
    "### Divide into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ----------------------------------------\n",
    "reports = df['reflection_answer_english'].tolist()\n",
    "print(f\"Loaded {len(reports)} (translated) documents for BERTopic modeling.\")\n",
    "# ----------------------------------------\n",
    "# Divide each report into sentences\n",
    "reports_sentences = [nltk.sent_tokenize(report) for report in reports]\n",
    "\n",
    "# Calculate the total number of sentences\n",
    "sentences_per_report = [len(report) for report in reports_sentences] #keep track of the number of sentences in each report (for further analysis)\n",
    "print(f\"Number of sentences in each report (mapping): {sentences_per_report}\")\n",
    "print(f\"Total number of sentences: {sum(sentences_per_report)}\")\n",
    "\n",
    "\n",
    "all_sentences = [sentence for report in reports_sentences for sentence in report]\n",
    "print(f\"Total number of sentences across all reports: {len(all_sentences)}\") #sanity check, should match the sum above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309675d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate stats for the distribution of sentences per report\n",
    "sentences_array = np.array(sentences_per_report)\n",
    "mean_sentences = np.mean(sentences_array)\n",
    "median_sentences = np.median(sentences_array)\n",
    "std_sentences = np.std(sentences_array)\n",
    "min_sentences = np.min(sentences_array)\n",
    "max_sentences = np.max(sentences_array)\n",
    "\n",
    "print(f\"Mean sentences per report: {mean_sentences:.2f}\")\n",
    "print(f\"Median sentences per report: {median_sentences}\")\n",
    "print(f\"Standard deviation: {std_sentences:.2f}\")\n",
    "print(f\"Minimum sentences in a report: {min_sentences}\")\n",
    "print(f\"Maximum sentences in a report: {max_sentences}\")\n",
    "\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sentences_array, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Sentences per Report')\n",
    "plt.xlabel('Number of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(sentences_array, vert=False)\n",
    "plt.title('Boxplot of Sentences per Report')\n",
    "plt.xlabel('Number of Sentences')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate outlier thresholds using IQR method\n",
    "Q1 = np.percentile(sentences_array, 25)\n",
    "Q3 = np.percentile(sentences_array, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find indices of outlier reports\n",
    "outlier_indices = np.where((sentences_array < lower_bound) | (sentences_array > upper_bound))[0]\n",
    "\n",
    "# Print number of outlier reports\n",
    "print(f\"Number of outlier reports (by sentence count): {len(outlier_indices)}\")\n",
    "\n",
    "# Print content of outlier reports\n",
    "for idx in outlier_indices:\n",
    "    print(f\"\\nReport index: {idx}, Sentence count: {sentences_array[idx]}\")\n",
    "    print(\"Sentences:\")\n",
    "    for sent in reports_sentences[idx]:\n",
    "        print(f\"- {sent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mosaicvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
